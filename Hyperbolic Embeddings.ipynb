{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd32c0c6-41df-4a24-99db-4c84f2372807",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24553852-53db-4cff-96b0-4c3318d8f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b717f0bc-d2d7-493e-be1b-f1d107925a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2,1,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03223427-5a9a-4d05-bcdf-48bbbb189aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c29499-3bfd-4edb-a11d-2937d6c8500c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6 // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e762e9-6d19-431e-8416-6fcc27571484",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04824feb-a6b0-4e89-a495-b1d072a5e661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from KGEmb.datasets.kg_dataset import KGDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "640480cf-4d2b-431f-a34e-c080a6eef7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './KGEmb/data/FB15K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1f041e1-07a3-4944-9e84-eba96684d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = './KGEmb/data/large_dummy_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49ec13b7-a611-4235-972e-b94d97a3cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KGDataset(dataset_path, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54acebb4-b196-48f4-a99a-c13c5d205318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 40, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d62766-870e-4d20-9e4b-7a7b4b8bb510",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_filters()['lhs'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569341f1-01ac-439e-984b-31fc5105cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.sizes = dataset.get_shape()\n",
    "\n",
    "# load data\n",
    "logging.info(\"\\t \" + str(dataset.get_shape()))\n",
    "train_examples = dataset.get_examples(\"train\")\n",
    "valid_examples = dataset.get_examples(\"valid\")\n",
    "test_examples = dataset.get_examples(\"test\")\n",
    "filters = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f527c70-3c7d-4a73-b2e6-94b1ce714fbd",
   "metadata": {},
   "source": [
    "# Dummy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b1913cb-4f5b-4c90-b1f7-b15b3aa736e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "# Create dummy directory\n",
    "os.makedirs(\"KGEmb/data/dummy_data\", exist_ok=True)\n",
    "\n",
    "# Sample triples (head, relation, tail)\n",
    "dummy_triples = {\n",
    "    \"train\": np.array([\n",
    "        [0, 0, 1],  # Paris (0) - capital_of (0) - France (1)\n",
    "        [2, 0, 3],  # Berlin (2) - capital_of - Germany (3)\n",
    "    ], dtype=np.int64),\n",
    "    \n",
    "    \"valid\": np.array([\n",
    "        [4, 0, 5]   # Rome (4) - capital_of - Italy (5)\n",
    "    ], dtype=np.int64),\n",
    "    \n",
    "    \"test\": np.array([\n",
    "        [6, 0, 7]   # Madrid (6) - capital_of - Spain (7)\n",
    "    ], dtype=np.int64)\n",
    "}\n",
    "\n",
    "# Create filters (using process.py logic)\n",
    "all_triples = np.concatenate(list(dummy_triples.values()))\n",
    "n_relations = 1  # Only capital_of relation\n",
    "\n",
    "def create_filters(triples, n_rels):\n",
    "    filters = {\"lhs\": {}, \"rhs\": {}}\n",
    "    for h, r, t in triples:\n",
    "        # Right-hand filters (h, r) -> [t]\n",
    "        key = (h, r)\n",
    "        filters[\"rhs\"].setdefault(key, []).append(t)\n",
    "        \n",
    "        # Left-hand filters (t, r + n_rels) -> [h]\n",
    "        inv_r = r + n_rels\n",
    "        inv_key = (t, inv_r)\n",
    "        filters[\"lhs\"].setdefault(inv_key, []).append(h)\n",
    "    return filters\n",
    "\n",
    "filters = create_filters(all_triples, n_relations)\n",
    "\n",
    "# Save files\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    with open(f\"KGEmb/data/dummy_data/{split}.pickle\", \"wb\") as f:\n",
    "        pkl.dump(dummy_triples[split], f)\n",
    "\n",
    "with open(\"KGEmb/data/dummy_data/to_skip.pickle\", \"wb\") as f:\n",
    "    pkl.dump(filters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91259232-9bc6-4100-a7aa-04965754a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Configuration\n",
    "ENTITIES = 100\n",
    "RELATIONS = 20\n",
    "SPLIT_SIZES = {\n",
    "    'train': 500,\n",
    "    'valid': 100,\n",
    "    'test': 100\n",
    "}\n",
    "DATA_PATH = \"KGEmb/data/large_dummy_data\"\n",
    "\n",
    "def generate_dummy_dataset():\n",
    "    os.makedirs(DATA_PATH, exist_ok=True)\n",
    "    \n",
    "    # Generate random triples\n",
    "    dataset = {}\n",
    "    for split, size in SPLIT_SIZES.items():\n",
    "        heads = np.random.randint(0, ENTITIES, size)\n",
    "        tails = np.random.randint(0, ENTITIES, size)\n",
    "        rels = np.random.randint(0, RELATIONS, size)\n",
    "        dataset[split] = np.stack([heads, rels, tails], axis=1).astype(np.int64)\n",
    "    \n",
    "    # Create filters\n",
    "    all_triples = np.concatenate(list(dataset.values()))\n",
    "    filters = {'lhs': defaultdict(list), 'rhs': defaultdict(list)}\n",
    "    \n",
    "    for h, r, t in all_triples:\n",
    "        # Right-hand filters\n",
    "        filters['rhs'][(h, r)].append(t)\n",
    "        # Left-hand filters (inverse relations)\n",
    "        filters['lhs'][(t, r + RELATIONS)].append(h)\n",
    "    \n",
    "    # Save files\n",
    "    for split in SPLIT_SIZES:\n",
    "        with open(f\"{DATA_PATH}/{split}.pickle\", \"wb\") as f:\n",
    "            pkl.dump(dataset[split], f)\n",
    "    \n",
    "    with open(f\"{DATA_PATH}/to_skip.pickle\", \"wb\") as f:\n",
    "        pkl.dump(filters, f)\n",
    "\n",
    "generate_dummy_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b95d4-cde5-4cf9-8c29-0b682ddbef7d",
   "metadata": {},
   "source": [
    "# Load saved KGEmb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7180d381-357d-4cc5-b292-11a8034db567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from KGEmb.models.hyperbolic import AttH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "446d6df1-389d-45c5-be70-40392c487cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs:\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"Knowledge Graph Embedding Model Configuration\"\"\"\n",
    "        self.rank = 1000                # Embedding dimension\n",
    "        self.learning_rate = 1e-1       # Initial learning rate\n",
    "        self.batch_size = 1000          # Training batch size\n",
    "        self.reg = 0.1                  # Regularization strength\n",
    "        self.max_epochs = 10            # Maximum training epochs\n",
    "        self.patience = 20              # Early stopping patience\n",
    "        self.debug = False              # Debug mode flag\n",
    "        self.dtype = 'double'           # Data type (float32/double)\n",
    "        self.neg_sample_size = 50       # Negative samples per positive\n",
    "        self.double_neg = True          # Use double negative sampling\n",
    "        self.bias = 'constant'          # Bias type in model\n",
    "        self.init_size = 1e-3           # Embedding initialization scale\n",
    "        self.multi_c = True             # Multiple curvatures (hyperbolic)\n",
    "        self.dropout = 0                # Dropout rate\n",
    "        self.sizes = dataset.get_shape()# (n_entities, n_relations) from dataset\n",
    "        self.gamma = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ModelArgs({vars(self)})\"\n",
    "model_args = ModelArgs(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f1f3b9f9-dc44-44c0-8483-5604bea35a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttH(\n",
       "  (entity): Embedding(100, 1000)\n",
       "  (rel): Embedding(40, 1000)\n",
       "  (bh): Embedding(100, 1)\n",
       "  (bt): Embedding(100, 1)\n",
       "  (rel_diag): Embedding(40, 2000)\n",
       "  (context_vec): Embedding(40, 1000)\n",
       "  (act): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AttH(model_args)\n",
    "model_path = '/Users/nathan/Documents/projects/curve_rag/KGEmb/logs/04_20/large_dummy_data/AttH_19_42_15/model.pt'\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7b09d4d1-e322-4347-87b4-5968c5f50eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all entity embeddings (n_entities x embedding_dim)\n",
    "entity_embeddings = model.entity.weight.data.cpu().numpy()\n",
    "\n",
    "# Get relation embeddings (n_relations x embedding_dim)\n",
    "relation_embeddings = model.rel.weight.data.cpu().numpy()\n",
    "\n",
    "# Save to file\n",
    "import numpy as np\n",
    "os.makedirs(\"./embeddings\", exist_ok=True)\n",
    "\n",
    "np.save(\"./embeddings/entity_emb.npy\", entity_embeddings)\n",
    "np.save(\"./embeddings/relation_emb.npy\", relation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ec3df32-a020-4e87-80f7-e7b1bc03ca3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10112191,  0.10021676,  0.10050033, ...,  0.08976474,\n",
       "         0.09802748, -0.10050702],\n",
       "       [-0.09734707, -0.09873435,  0.09664864, ..., -0.09972362,\n",
       "         0.10046091,  0.09921627],\n",
       "       [ 0.09772456, -0.10011804,  0.09926715, ...,  0.09629074,\n",
       "         0.09508758, -0.10133961],\n",
       "       ...,\n",
       "       [-0.09776815, -0.09312213, -0.09745788, ...,  0.07169162,\n",
       "        -0.08136066,  0.09823062],\n",
       "       [ 0.10166223, -0.0786465 , -0.10073881, ..., -0.09493197,\n",
       "        -0.05466219,  0.09033944],\n",
       "       [ 0.10023003, -0.09891245,  0.09755384, ..., -0.10133544,\n",
       "        -0.09676388,  0.0957422 ]], shape=(100, 1000))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e801bca3-fc5a-450c-8537-5f8e061c3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model.eval()\n",
    "\n",
    "# Example prediction: (h, r) -> predict t\n",
    "def predict_tail(h, r, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        scores, _ = model.get_queries(torch.tensor([[h, r, 0]]))\n",
    "        #print(type(scores[0]), scores)\n",
    "        scores = scores[0]\n",
    "        values, indices = torch.topk(scores, k=top_k)\n",
    "        return [(idx, score.item()) \n",
    "                for idx, score in zip(indices[0], values[0])]\n",
    "\n",
    "# Usage\n",
    "predictions = predict_tail(h=0, r=0)  # (Paris, capital_of) -> top 5 countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2e8df9ed-427a-46a8-b916-21391d6a5f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor(74), 0.05480053575637172),\n",
       " (tensor(50), 0.054452233924053406),\n",
       " (tensor(21), 0.05377454100519527),\n",
       " (tensor(584), 0.05350505764806299),\n",
       " (tensor(541), 0.053469977380110126)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
